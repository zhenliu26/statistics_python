import math
import numpy as np
import pandas as pd

# acquisite the data
source_path = "https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DA0101EN/auto.csv"
df = pd.read_csv(source_path, header=None) # no header, so header=None;
print(df.head(5)) # show first 5 rows of data
print('hello world')

# if you need to add header for the data
headers = ["symboling","normalized-losses","make","fuel-type","aspiration", "num-of-doors","body-style",
         "drive-wheels","engine-location","wheel-base", "length","width","height","curb-weight","engine-type",
         "num-of-cylinders", "engine-size","fuel-system","bore","stroke","compression-ratio","horsepower",
         "peak-rpm","city-mpg","highway-mpg","price"]

df.columns = headers
print(df.head(5))

# sometimes there is NAN in the data, we can drop them
print(df.dropna(subset=["price"],axis=0)) # 0 is row, 1 is column

# export csv
# df.to_csv("automobile.csv",index=False)

# read data type
print(df.dtypes)

# get the mathematics for each column
print(df.describe()) # it can exclude nan, if want all, include = "all"

# get the stat for specific columns
print(df[['symboling','city-mpg']].describe())

#### chapter 2 -- Data wrangling
# how to work with missing data
# 1. dentify missing data;
# 2. deal with missing data;
# 3. correct data format.

# convert "?" to NAN
df.replace("?",np.nan,inplace=True)
print(df.head(5))

# evaluating for missing data
missing_data = df.isnull()
print(missing_data.head(5))
for column in missing_data.columns.values.tolist():
    print(missing_data[column].value_counts()) # calculate the number of pieces of missing data for each column

# use three kinds of method to correct data
# drop data
# a. drop the whole row
# b. drop the whole column
# replace data
# a. replace it by mean
# b. replace it by frequency
# c. replace it based on other functions


# replace by mean value
avg_norm_loss = df["normalized-losses"].astype("float").mean(axis=0)
df["normalized-losses"].replace(np.nan,avg_norm_loss,inplace=True)
print(df['normalized-losses'])

# data standardization
# Data is usually collected from different agencies with different formats.
# (Data Standardization is also a term for a particular type of data normalization, where we subtract the mean and divide by the standard deviation)

# add a new column
df["city-L/100km"]=235/df["city-mpg"]
print(df.head(5))

# rename the old column
df['highway-mpg']=235/df['highway-mpg']
df.rename(columns={'"highway-mpd"':'highway-L/100km'},inplace=True)

# data normalization
# divided by the max value of the column
df['height'] = df['height']/df['height'].max()
print(df['height'])

# binning: Binning is a process of transforming continuous numerical variables into discrete categorical 'bins', for grouped analysis.
bins = np.linspace(min(df["horsepower"].astype('float')), max(df["horsepower"].astype('float')), 4)
print(bins)
group_names = ['Low', 'Medium', 'High']
df['horsepower-binned'] = pd.cut(df['horsepower'].astype('float'), bins, labels=group_names, include_lowest=True)
print(df[['horsepower','horsepower-binned']].head(20))
df['horsepower']=df['horsepower'].astype('float')
print(df['horsepower'].dtypes)

# An indicator variable (or dummy variable) is a numerical variable used to label categories. They are called 'dummies' because the numbers themselves don't have inherent meaning.
dummy_variable_1 = pd.get_dummies(df["fuel-type"])

# merge data frame "df" and "dummy_variable_1"
df = pd.concat([df, dummy_variable_1], axis=1) #add two columns to the sheet

# drop original column "fuel-type" from "df"
df.drop("fuel-type", axis = 1, inplace=True) # delete the source
print(df['diesel'].value_counts())


